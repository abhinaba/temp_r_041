We thank reviewers tVcM, 53fB, and tDsL for their constructive feedback. We provide anonymized code and complete experimental results at [anonymous.4open.science link].

Response to Reviewer tVcM

Operator validity: We implemented a retrieval-infill operator that replaces attribution-identified tokens with corpus-sampled alternatives (leave-one-out sampling, label blacklisting), preserving distributional properties while disrupting explanation content. We evaluated across 7 models x 4 datasets x 2 extractors (56 configurations, n=500 each). Delete and retrieval operators agree on qualitative faithfulness conclusions in 72% of configurations (threshold 0.55). The 28% disagreement is systematic: on long texts (IMDB), retrieval yields higher win rates (mean 0.751 vs 0.667, +0.083), suggesting deletion underestimates faithfulness due to OOD artifacts. On short texts (SST-2, AG News), both operators produce consistent conclusions (mean difference <0.04). Model ranking is preserved across operators: Qwen2.5-7B achieves the highest win rates under both operators.

Language coverage: We extended evaluation to Arabic and Turkish (in addition to German, French, Hindi, Chinese), testing all 7 models with both extractors (84 configurations). Arabic shows strong attribution signals (deepseek attention WR=0.800, GPT-2=0.717). Turkish results correlate with tokenizer quality: Qwen2.5-7B achieves 0.816 attention WR, while GPT-2 (2.4x token expansion) achieves 0.432.

Self-explanation alignment: ICE evaluates whether identified tokens are *causally* important (intervention-based), which is complementary to self-explanation faithfulness (whether generated rationales reflect internal reasoning). Our preliminary CoT extension bridges these: we tested 6 models on SST-2, AG News, e-SNLI, and GSM8K using the same necessity/sufficiency framework. Most CoT reasoning is "lucky tokens" -- sufficient but not necessary, suggesting post-hoc rationalization. On GSM8K, all models show "random guess" (Nec<0.4, Suf<0.13), indicating CoT tokens are not causally used despite containing valid-looking reasoning. This demonstrates ICE can evaluate both attribution-based and generation-based explanations under a unified statistical framework. Code and results are provided.

Response to Reviewer 53fB

We appreciate the reviewer's recognition of the methodology's soundness (Score: 4) and agree that the presentation needed significant improvement. We undertook a major revision addressing all concerns: (1) reduced Table/Figure density by consolidating supplementary results into the appendix, keeping only the core comparison tables; (2) added a concrete sentiment classification example (Section 3.1) illustrating the intervention rationale with specific tokens and predictions; (3) introduced all acronyms at first use (NSR, NSD, etc.); (4) corrected all noted grammatical issues (lines 379, 391, 450); (5) added bold formatting for best values in Tables 5-6. The narrative now follows a clear thread: motivation (Section 1) -> statistical framework (Section 3.1-3.2) -> implementation (Section 3.3-3.5) -> results organized by research question.

Response to Reviewer tDsL

Methodological clarity: We restructured Section 3 following the reviewer's specific suggestion, presenting the conceptual protocol (Sections 3.4-3.5 content) before operator implementation details (Sections 3.2-3.3 content). A concrete example now illustrates the full pipeline: given a sentiment prediction, ICE (a) identifies top-k tokens via attribution, (b) applies matched intervention (deletion or retrieval), (c) compares score retention against random baselines of identical size, and (d) tests statistical significance via Wilcoxon signed-rank test.

Prior work distinction: The reviewer correctly notes the apparent tension between "underexplored" (line 042) and the 110+ method survey (line 091). We clarify: the survey covers methods for *generating* explanations (attention, gradient, LIME, etc.), while ICE addresses *evaluation* of whether these explanations are faithful -- a distinct and underexplored problem. Existing evaluations rely on point estimates without hypothesis testing; ICE provides the missing statistical foundation.

Cross-architecture generality: BERT is included in Section 3.2 because ICE is architecture-agnostic by design, not LLM-specific. To demonstrate this concretely, we evaluated BERT-base with 4 attribution methods (attention, gradient, integrated gradients, LIME) across 3 datasets using retrieval infill. BERT achieves win rates up to 0.788 (IMDB attention), confirming ICE generalizes across encoder and decoder architectures. This also validates the retrieval operator beyond LLMs.

All code, 56 LLM retrieval results, 84 multilingual results, encoder results, and 20 CoT result files are provided for full reproducibility.
